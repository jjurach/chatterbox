# Task 4.13: Performance Optimisation

**Date:** 2026-02-20
**Status:** Complete

## Summary

Profiled and optimised the agentic conversation loop for latency and
responsiveness.  Two concrete improvements were implemented:

1. **Parallel tool dispatch** — multiple tool calls requested by the LLM in a
   single response are now dispatched concurrently via `asyncio.gather` instead
   of sequentially.
2. **TTL-based tool result cache** — a new `ToolResultCache` / `CachingDispatcher`
   API lets callers cache tool results to avoid redundant API calls for
   repeated queries (e.g. "what's the weather?" asked twice within 5 minutes).
3. **Timing instrumentation** — wall-clock timings are logged at `DEBUG` level
   for each LLM call and each tool-dispatch batch, and at `INFO` level for the
   total turn latency.  This makes profiling visible without running a separate
   profiler.

## Profiling Analysis

The agentic loop's main latency contributors (in descending order):

| Phase | Typical cost | Notes |
|---|---|---|
| LLM API call | 1–10 s | Dominated by network + model inference; not reducible at our layer |
| Tool execution | 0.1–2 s per tool | Previously sequential; now concurrent |
| Tool API calls (weather, etc.) | 0.1–1 s | Cacheable — avoids redundant HTTP requests |
| Loop overhead (message building, JSON parsing) | < 5 ms | Negligible |

## Changes

### `src/chatterbox/conversation/loop.py`

* Added `import asyncio` and `import time`.
* `_dispatch_tool_calls` rewritten to use `asyncio.gather` — all tool
  coroutines run simultaneously.  Result order is preserved (matches
  `tool_calls` list order).
* Wall-clock timing added via `time.monotonic()`:
  - Per-LLM-call: logged at DEBUG as `"LLM call N took X.XXXs (finish_reason=...)"`
  - Per-tool-batch: logged at DEBUG as `"Dispatched N tool(s) concurrently in X.XXXs"`
  - Total turn: logged at INFO as `"Loop complete after N iteration(s) in X.XXXs"`

### `src/chatterbox/conversation/tools/cache.py` (new)

* `ToolResultCache(ttl: float = 300.0)` — plain-dict TTL store keyed by
  `(tool_name, JSON-sorted-args)`.
  - `get(name, args)` → `str | None` (returns `None` on miss or expiry; evicts expired entries lazily)
  - `put(name, args, result)` — stores with expiry timestamp
  - `invalidate(name, args=None)` — remove one entry or all entries for a tool
  - `clear()` — flush all entries
  - `len(cache)` — entry count (including not-yet-evicted expired entries)
  - TTL ≤ 0 disables caching (every `get` returns `None`, `put` is a no-op)
* `CachingDispatcher(inner, cache)` — transparent wrapper around any
  dispatcher callable.  On a cache miss, delegates to `inner` and stores the
  result; on a hit, returns the cached value without calling `inner`.

## Tests

### `tests/unit/test_conversation/test_loop.py`

* `test_tool_calls_dispatched_concurrently` — verifies concurrent execution
  by recording `start:<name>` / `end:<name>` events and asserting that both
  tools start before either finishes.
* `test_concurrent_tool_dispatch_preserves_order` — verifies that tool result
  messages are returned in the same order as the original `tool_calls` list.

### `tests/unit/test_conversation/test_cache.py` (new, 20 tests)

* `ToolResultCache`: miss, hit, multiple entries, arg ordering, TTL expiry
  (using `monkeypatch` on `time.monotonic`), zero/negative TTL, `invalidate`,
  `clear`, `len`.
* `CachingDispatcher`: miss delegates to inner, hit avoids re-call, different
  args each get their own cache slot, result stored after miss, zero-TTL always
  calls inner, expired entries are re-fetched.

## Performance Impact

Before (sequential dispatch with 2 tools, each taking 500 ms):

```
Total tool phase: 1000 ms
```

After (concurrent dispatch):

```
Total tool phase: ~500 ms  (limited by slowest tool, not sum)
```

For N simultaneous tool calls the reduction scales as:
`sequential_time = N × avg_tool_latency` → `concurrent_time = max_tool_latency`

## Deferred / Out of Scope

* **LLM response streaming** — would reduce time-to-first-token for voice
  responses, but requires protocol changes in `LLMProvider.complete` (streaming
  vs. non-streaming distinction, partial message assembly, early tool-call
  detection).  Deferred to a future task if latency profiling shows it
  necessary after cache + parallel dispatch are in place.
* **Persistent cross-session tool cache** — the `ToolResultCache` is in-memory
  and is lost on restart.  A Redis or SQLite backend is deferred to Epic 5
  alongside the context persistence work (see `docs/epic5-context-persistence-proposal.md`).
