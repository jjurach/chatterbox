# Task 4.2: Select and Evaluate Agentic Framework

**Status:** Completed
**Created:** 2026-02-20
**Epic:** Epic 4 — LLM Integration with Tool Calling
**Depends On:** Task 4.1 (Complete — HA Conversation Flow research)

---

## Context

Task 4.1 established that the LLM integrates into HA as a **custom ConversationEntity**,
not as a Wyoming endpoint. This changes the scope of the framework evaluation: instead
of evaluating frameworks for a standalone agentic service, we are evaluating what to
use **inside a ConversationEntity's `async_process()` method** for the tool-calling loop.

The HA ConversationEntity pattern:
1. HA calls `async_process(user_input, chat_log)` after STT
2. Entity calls LLM with tools (provided by HA's LLM API helper)
3. Entity executes tool calls from LLM, feeds results back
4. Entity returns final text response to HA
5. HA sends text to TTS

---

## Framework Options Evaluated

### Option 1: LangChain Classic (existing agent.py)
- Synchronous ReAct agent using `langchain_classic`
- **Status:** Already in codebase as `agent.py`
- **Assessment:** ❌ Not suitable — synchronous, doesn't fit HA's async architecture, legacy API

### Option 2: LangGraph (1.0.7 installed)
- Graph-based agent with state management
- **Assessment:** ⚠️ Overkill — adds graph complexity for a simple linear tool-calling loop.
  Valuable for multi-agent, branching, or checkpoint scenarios, not single-agent linear flow.

### Option 3: Custom Minimal Async Loop (OpenAI SDK)
- A simple `while True` loop using `openai.AsyncOpenAI` (works with Ollama, OpenAI, Claude proxy)
- **Assessment:** ✅ Recommended — async-native, simple, testable, already-installed dep

### Option 4: Raw Anthropic SDK
- Direct `anthropic.AsyncAnthropic` calls
- **Assessment:** ❌ Vendor-locked, not installed, Ollama compatibility requires separate config

---

## Decision: Option 3 — Custom Minimal Async Loop

The tool-calling pattern is:
```
call LLM → if tool_call → dispatch → feed results → repeat → return text
```
This is a simple loop, not a graph. LangGraph is available as a future option if
multi-agent complexity is needed (e.g., parallel tool execution, branching paths).

---

## Implementation Plan

### Step 1: Write framework evaluation document
- `docs/agentic-framework-evaluation.md`
- Documents all options, evaluation criteria, decision rationale

### Step 2: Create `src/chatterbox/conversation/` package
Files:
- `__init__.py` — public exports
- `providers.py` — `LLMProvider` Protocol + `OpenAICompatibleProvider`
- `loop.py` — `AgenticLoop` class (the tool-calling engine)
- `entity.py` — `ChatterboxConversationEntity` skeleton (HA ConversationEntity interface)

### Step 3: Update `pyproject.toml`
- Add `chatterbox.conversation` to the packages list

### Step 4: Create unit tests
- `tests/unit/test_conversation/`
- `test_loop.py` — tests for AgenticLoop (mock LLM provider)
- `test_providers.py` — tests for OpenAICompatibleProvider config

### Step 5: Create change log
- `dev_notes/changes/2026-02-20_*_task-4.2-framework-evaluation.md`

### Step 6: Mark project plan complete

---

## Definition of Done

- [ ] Framework evaluation doc created in `docs/`
- [ ] `src/chatterbox/conversation/` package exists with all 4 files
- [ ] `pyproject.toml` updated with new package
- [ ] Unit tests pass (all in `tests/unit/test_conversation/`)
- [ ] Change log written with verification output
- [ ] Project plan status set to `Completed`
